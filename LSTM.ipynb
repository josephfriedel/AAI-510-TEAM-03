{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9bsbh5fwwyihsjgU8Sfci",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/josephfriedel/AAI-510-TEAM-03/blob/main/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kvTFNYY7MXT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pretty_midi\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding, Masking\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pretty_midi\n",
        "import librosa\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the directory\n",
        "bach_dir = 'music_data/midiclassics/Bach'\n",
        "beethoven_dir = 'music_data/midiclassics/Beethoven'\n",
        "chopin_dir = 'music_data/midiclassics/Chopin'\n",
        "mozart_dir = 'music_data/midiclassics/Mozart'\n",
        "\n",
        "composer_dirs = [bach_dir, beethoven_dir, chopin_dir, mozart_dir]"
      ],
      "metadata": {
        "id": "9Jn2CMgu7iji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#jfXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXxx\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# Assigning directory paths to variables\n",
        "bach_dir = r'C:\\Users\\josep\\git\\git2\\AAI-511-final-project\\music_data\\midiclassics\\Bach'\n",
        "beethoven_dir = r'C:\\Users\\josep\\git\\git2\\AAI-511-final-project\\music_data\\midiclassics\\Beethoven'\n",
        "chopin_dir = r'C:\\Users\\josep\\git\\git2\\AAI-511-final-project\\music_data\\midiclassics\\Chopin'\n",
        "mozart_dir = r'C:\\Users\\josep\\git\\git2\\AAI-511-final-project\\music_data\\midiclassics\\Mozart'\n",
        "\n",
        "# Define the directories and their sizes\n",
        "directories = {\n",
        "    bach_dir: 925,\n",
        "    beethoven_dir: 212,\n",
        "    chopin_dir: 136,\n",
        "    mozart_dir: 257\n",
        "}\n",
        "\n",
        "# Find the largest directory size\n",
        "max_size = max(directories.values())\n",
        "\n",
        "# Function to balance a directory by increasing the number of files\n",
        "def balance_directory(directory, target_size):\n",
        "    files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
        "    current_size = len(files)\n",
        "\n",
        "    if current_size < target_size:\n",
        "        while current_size < target_size:\n",
        "            file_to_duplicate = random.choice(files)\n",
        "            base, extension = os.path.splitext(file_to_duplicate)\n",
        "            new_file = f\"{base}_copy_{current_size}{extension}\"\n",
        "            try:\n",
        "                shutil.copy(os.path.join(directory, file_to_duplicate), os.path.join(directory, new_file))\n",
        "                files.append(new_file)\n",
        "                current_size += 1\n",
        "            except PermissionError as e:\n",
        "                print(f\"PermissionError: {e}. Skipping {file_to_duplicate}\")\n",
        "\n",
        "# Balance each directory\n",
        "for dir_name in directories:\n",
        "    balance_directory(dir_name, max_size)"
      ],
      "metadata": {
        "id": "EvIjPqGg7l9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(midi_file):\n",
        "    midi_data = pretty_midi.PrettyMIDI(midi_file)\n",
        "\n",
        "    # Extract note pitches\n",
        "    pitches = []\n",
        "    for instrument in midi_data.instruments:\n",
        "        for note in instrument.notes:\n",
        "            pitches.append(note.pitch)\n",
        "\n",
        "    # Extract note durations\n",
        "    durations = []\n",
        "    for instrument in midi_data.instruments:\n",
        "        for note in instrument.notes:\n",
        "            durations.append(note.end - note.start)\n",
        "\n",
        "    # Extract note velocities\n",
        "    velocities = []\n",
        "    for instrument in midi_data.instruments:\n",
        "        for note in instrument.notes:\n",
        "            velocities.append(note.velocity)\n",
        "\n",
        "    # Compute average and standard deviation for features\n",
        "    features = {\n",
        "        'avg_pitch': np.mean(pitches),\n",
        "        'std_pitch': np.std(pitches),\n",
        "        'avg_duration': np.mean(durations),\n",
        "        'std_duration': np.std(durations),\n",
        "        'avg_velocity': np.mean(velocities),\n",
        "        'std_velocity': np.std(velocities),\n",
        "        'note_vector': pitches,  # Add the sequence of note pitches as a feature\n",
        "        'duration_vector': durations,  # Add the sequence of note durations as a feature\n",
        "        'velocity_vector': velocities  # Add the sequence of note velocities as a feature\n",
        "    }\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "def load_data(directory, current_label, data, success_files, exception_files):\n",
        "    print(f'Loading {directory} for {current_label}...')\n",
        "       # Use os.walk to iterate over each subdirectory in the current directory\n",
        "        # 'root' is the path to the current directory\n",
        "        # 'dirs' is a list of the names of the subdirectories in 'root'\n",
        "        # 'files' is a list of the names of the non-directory files in 'root'\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            # Check if the file is a MIDI file\n",
        "            if file.endswith('.mid'):\n",
        "                file_path = os.path.join(root, file)\n",
        "                print(f'Processing {file}...')\n",
        "                print(f'File path: {file_path}')\n",
        "                try:\n",
        "                    features = extract_features(file_path)\n",
        "                    features['composer'] = current_label\n",
        "                    data.append(features)\n",
        "                    success_files.append(file_path)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {file_path}: {e}\")\n",
        "                    exception_files.append(file_path)\n",
        "\n",
        "def create_dataframe(data, max_sequence_length = 100):\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    df['composer'] = df['composer'].map({0: 'Bach', 1: 'Beethoven', 2: 'Chopin', 3: 'Mozart'})\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Pad sequences to a fixed length\n",
        "    note_vectors = pad_sequences(df['note_vector'].tolist(), maxlen=max_sequence_length, padding='post')\n",
        "    duration_vectors = pad_sequences(df['duration_vector'].tolist(), maxlen=max_sequence_length, padding='post')\n",
        "    velocity_vectors = pad_sequences(df['velocity_vector'].tolist(), maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "    # Add padded sequences to the dataframe\n",
        "    df['note_vector'] = list(note_vectors)\n",
        "    df['duration_vector'] = list(duration_vectors)\n",
        "    df['velocity_vector'] = list(velocity_vectors)\n",
        "    return df\n",
        "\n",
        "data = []\n",
        "success_files = []\n",
        "exception_files = []\n",
        "\n",
        "# Load data\n",
        "for current_label, composer_dir in enumerate(composer_dirs):\n",
        "    load_data(composer_dir, current_label, data, success_files, exception_files)\n",
        "\n",
        "# Create dataframe\n",
        "df = create_dataframe(data)\n",
        "\n",
        "print(f\"Success files: {len(success_files)}\")\n",
        "print(f\"Exception files: {len(exception_files)}\")\n",
        "\n",
        "# Print shape of data\n",
        "print(f\"Data shape: {df.shape}\")\n",
        "\n",
        "# List all exceptions\n",
        "for file in exception_files:\n",
        "    print(file)\n",
        "\n",
        "display(df.head())"
      ],
      "metadata": {
        "id": "Py6oVybS7uQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#jfXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXxxxx\n",
        "\n",
        "# Prepare the dataset\n",
        "X = df.drop(columns=['composer'])\n",
        "y = df['composer']\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Use RandomUnderSampler to handle class imbalance\n",
        "#rus = RandomUnderSampler(random_state=42)\n",
        "X_resampled, y_resampled = (X, y_encoded)\n",
        "\n",
        "# Convert resampled data back to DataFrame for easier handling\n",
        "X_resampled_df = pd.DataFrame(X_resampled, columns=X.columns)\n",
        "y_resampled_df = pd.Series(y_resampled, name='composer')\n",
        "\n",
        "# Verify class distribution after undersampling\n",
        "print(y_resampled_df.value_counts())"
      ],
      "metadata": {
        "id": "7wkKEkpr8MKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the LSTM model\n",
        "model = Sequential([\n",
        "    LSTM(64, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True),\n",
        "    LSTM(64),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(len(label_encoder.classes_), activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "sFfexkub8TWa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}